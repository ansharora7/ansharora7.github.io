---
layout: single
title: "News"
permalink: /news/
---

- ** April 2025** — Started as a **Graduate Researcher at Google DeepMind**, contributing to cutting-edge work on **meta-optimization** and **adaptive model ensembling**. The research aims to improve training efficiency and generalization across deep learning tasks, with a focus on scalable architectures and long-range task adaptation.

- ** October 2024** — Published a blog post on **Medium** breaking down our ACL 2024 paper, covering the inspiration, methodology, and real-world implications of our approach to defending against backdoored NLP models.  
  ➡️ [Read: “Here’s a Free Lunch: Sanitizing Backdoored Models with Model Merge”](https://medium.com/p/b753ab9a5b59)

- ** February 2024** — Our work *"Here’s a Free Lunch: Sanitizing Backdoored Models with Model Merge"* was accepted at **ACL 2024 (Findings)**. The paper introduces a defense that merges multiple models at inference-time, significantly reducing the effectiveness of NLP backdoor attacks (75%+ drop in attack success rate), while preserving clean accuracy. Evaluated across SST-2, QNLI, Amazon, and IMDB with BERT, RoBERTa, and Llama2/Mistral LLMs.
